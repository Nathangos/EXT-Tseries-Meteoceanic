---
output:
  html_document: default
  pdf_document: default
---

### Import des fonctions.

```{r}
rm(list=ls())
set.seed(133)
require(reshape2)
require(ggplot2)
require(gridExtra)
require(grid)
require(dplyr)
require(threshr)
require(extRemes)
require(ggside)
require(FactoMineR)
require(POT)
require(VineCopula)
require(goftest)
require(scales)
require(threshr)
```

# Setup
```{r}
### Import of the functions
source("functions_toy.R")

# Parameters chosen. 
####################

##### Number of generated time series. 
M<-5000

##### Weight put on the distribution tail
qtile_ext<-0.95

##### Mean and standard deviation of beta
mu<-0
sigma_N<-2**(1/2)

##### Parameters of the excesses
params_LAW<-list(mu,sigma_N)
ech_ev<-1
gam_ev<--0.10

seuil<-qnorm(p = qtile_ext,mean = mu,sd = sigma_N)
seuil
```
```{r}
vecteur_u<-runif(M)
vect_beta<-sapply(vecteur_u,Generateur_beta,quant=qtile_ext,params_lois=params_LAW,
       gam_ev=gam_ev,ech_ev=ech_ev,seuil_ev=seuil)
plot(density(vect_beta))
```

# Generate the residuals.

```{r}
# Generation curves ------------------------------------------------------
c<-0
L_courbe<-37
begg<-0
end<-1

# change the peak position ------------------------------------------------
Time_step<-seq.int(from = begg,to = end,length.out = L_courbe)
DPHASE<-TRUE

# choose the toy dataset ------------------------------------------------
CHOICE<-"POLY"

```


## Choice of the toy dataset
We want to obtain curves with a "triangular pattern", a symmetric curve reaching its peak during the time window. For each time series, we observe the evolution of a variable according to $t$ taking values between $0$ and $1$. 

We want that having an extreme value of $\beta$ does change the shape of the time series. We can use the following formula obtain this type of result :
$$
\varepsilon(t)=\text{sign}(\beta)*\frac{|\beta|}{\sqrt{2\pi}}\exp(-\beta^{2}\frac{(t-\mu)^{2}}{2})

$$

For large values of $\beta$, we will clearly notice a peak when $t=\mu$ whereas we will have flat curves if $\beta$ is small. As residuals must be centered, we consider the sign of $\beta$ to obtain negative values for the curves. 

```{r}
summary(vect_beta)
```


```{r}
if(CHOICE=="POLY"){
  vect_courbes<-t(sapply(vect_beta,FUN = Generation_curves_poly,
       L_courbe=L_courbe,begg=begg,end=end,dephase=DPHASE,c=c))
}
if(CHOICE=="GAUSS"){
  vect_courbes<-t(sapply(vect_beta,Generation_curves_normal_LAW,
      L_courbe=L_courbe,begg=begg,end=end,dephase=DPHASE))
}

```

```{r}
Q_decomp<-qr(vect_courbes)
Q_decomp$rank
```

```{r}
Nlin_indep<-vect_courbes[,1:Q_decomp$rank]
XtX<-t(Nlin_indep)%*%Nlin_indep

```


### Illustration 
```{r}
Nshown<-100
percent_studied<-0.05
Q1<-as.numeric(apply(X = vect_courbes,MARGIN = 2,
          FUN = function(x){return(quantile(x,percent_studied/2))}))
Q2<-as.numeric(apply(X = vect_courbes,MARGIN = 2,
          FUN = function(x){return(quantile(x,1-percent_studied/2))}))

Mean<-colMeans(vect_courbes)
inds_taken<-sample(c(1:M),size = Nshown)
Curves_seen<-vect_courbes[inds_taken,]

```

```{r}
Melteur<-melt(t(Curves_seen))
colnames(Melteur)<-c("Time","number","value")
Melteur$number<-as.character(Melteur$number)
Melteur$Q1<-Q1[Melteur$Time]
Melteur$Q2<-Q2[Melteur$Time]
Melteur$mean_curve<-Mean[Melteur$Time]
ggplot(data=Melteur,aes(x=Time,y=value,col=number,
                        group=interaction(number),
                        ))+
  geom_ribbon(mapping=aes(ymin=Q1,ymax=Q2,linetype="confidence_band"),
              alpha=0.02,
              fill="grey",col="darkblue")+
  ylab("value (m)")+
  geom_line(aes(y=mean_curve,linetype="mean"),col="red")+
  geom_line(alpha=0.7)+
  guides(col="none")+
  scale_linetype_manual("Legend",values=c("confidence_band"=2, 
                                          "mean"=5))+
  ggtitle(paste0(Nshown," curves shown among the ",M," simulated"))
```

```{r}
POT::chimeas(data=cbind(vect_courbes[,1],
                        vect_courbes[,19]))
```


##1) Initialisation
```{r}
mid<-(end+begg)/2
vect_time<-seq.int(from = begg,to = end,length.out = L_courbe)
D<--abs(mid-vect_time)
XO<-D-min(D)+0.5
plot(c(1:37),XO,main="Initial time series",
     xlab="Time",ylab="Value")
```

##2) AR(1) model at each time
```{r}
ALPHA<-0.5
X_variable<-matrix(NA,nrow = M,ncol = L_courbe)
X_variable[1,]<-XO
for(j in c(2:M)){
  #remarque de Jeremy, rajouter un bruit.
  X_variable[j,]<-X_variable[j-1,]*ALPHA+vect_courbes[j,]
}
```

```{r}
matplot(t(X_variable),type="l",xlab="Time",ylab="value",
        main="Generated time series X")
```

# (I) Construction of the polar coordinates representation

##1) Applying an auto-regressive model at each time
```{r}
# degree of the ar model.
P<-1
Residuals_AR<-matrix(NA,nrow = M,ncol = L_courbe)
Df_reg_ar<-matrix(NA,nrow=L_courbe,ncol=P+1)
for(j in c(1:L_courbe)){
  series<-X_variable[,j]
  MODEL_AR<-arima(series,order=c(P,0,0),include.mean = TRUE)
  Df_reg_ar[j,]<-unlist(MODEL_AR$coef)
  Residuals_AR[,j]<-MODEL_AR$residuals
}

```


```{r}
P<-1
Df_reg_ar<-as.data.frame(Df_reg_ar)
colnames(Df_reg_ar)<-c("Intercept")
Name_cols_modele_lin<-c()
for(j in c(1:P)){
    Name_cols_modele_lin<-c(Name_cols_modele_lin,paste0("coeff_ar",j))
}
colnames(Df_reg_ar)<-c(Name_cols_modele_lin,"Intercept")

```

As we apply our model at each time, we can observe the residuals obtained.
```{r}
par(mfrow=c(1,2))
acf(Residuals_AR[,1])
pacf(Residuals_AR[,1])
par(mfrow=c(1,1))
```


We reduce the correlation level. We can analyse the type of extremes found in our data. 
##2) Extreme value diagnostic
```{r}
### Type of extremes
L2<-apply(X = Residuals_AR,MARGIN = 1,FUN = calcul_norme_L2)
L<-500
vect_k<-c(20:L)

Graphics_estimators_gamma(series = L2,vect_k = vect_k,
                             Title_graphic  = "Extreme of L2 before transformation")
```
As we see that the type of extreme is not convenient, we need to apply a marginal transformation. The latter uses as parameter a threshold $u_{t}$ defined at each time t.
We can use several tools to identify relevant thresholds such as the mrlplot and the tcplot. 

```{r}
j<-1
Quantile_level<-0.95
Chosen_leven<-quantile(series,Quantile_level)
series<-Residuals_AR[,j]
Min_u<-quantile(series,0.75)
Max_u<-quantile(series,0.98)
NB_TH_for_graphs<-100
POT::tcplot(series,ask = FALSE,u.range = c(Min_u,Max_u),
            which = 1,nt =NB_TH_for_graphs)
abline(v=quantile(series,qtile_ext),col="red")
POT::tcplot(series,ask = FALSE,u.range = c(Min_u,Max_u),
            which = 2,nt = NB_TH_for_graphs)
abline(v=quantile(series,qtile_ext),col="red")
```
If the parameters obtained on the tcplot are relatively stable for the selected threshold, we can use it in our transformation. We can also use the mean residual life plot to go further. 
```{r}
POT::mrlplot(data = series,ask = FALSE,u.range = c(Min_u,Max_u),
            nt = NB_TH_for_graphs)
abline(v=quantile(series,qtile_ext),col="red")
```
If the function becomes linear near the threshold we selected, it means that we made a good choice. We can now use the quantile level as a parameter of our function. 
## 3) Applying the marginal transformation
```{r}
### List of quantile
p_U<-rep(0.10,37)

### Density parameter for the estimation of the bulk.
n.dens<-2^(14)

Vecteur_DIM<-c(1:ncol(Residuals_AR))
Margi_transformation<-as.data.frame(t(sapply(Vecteur_DIM,f_marginales_all_Pareto,
                                             data_to_tf=Residuals_AR,p_u=p_U,
                                     n.dens=n.dens)))
```

```{r}
K_dens<-Margi_transformation$kernel_dens
Pareto<-do.call(cbind.data.frame,Margi_transformation$obs)
Unif<-1-(Pareto)^(-1)
Frechet<--1/log(Unif)
colnames(Frechet)<-c(1:ncol(Frechet))

```

```{r}
L_EVT<-list("gamma"=Margi_transformation$gamma,
              "scale"=Margi_transformation$scale,
              "threshold"=Margi_transformation$threshold,
              "p_u"=Margi_transformation$p_u)
```

We obtain very large values as we have now Frechet marginals. 
```{r}
### Illustration 
matplot(t(Frechet),type="l",ylab="value",xlab="Time",
        main="Time series on the Frechet scale",log="y")
```
We can verify that we obtained Frechet margins with our transformations. 
```{r}
###
test_ad<-rep(NA,ncol(Frechet))
for(t in c(1:ncol(Frechet))){
  test_ad[t]<-goftest::ad.test(x = Frechet[,t],null = extRemes::"pevd",
                               loc=1,shape=1,scale=1,type="GEV")$p.value
}
plot(c(1:ncol(Frechet)),test_ad
     ,xlab="Time",ylab="p value",main="Anderson-Darling test")
```
We use again the L2 norm to analyse the type of extremes in our data. 
```{r}
L2<-apply(X = Frechet,MARGIN = 1,FUN = calcul_norme_L2)
Graphics_estimators_gamma(series = L2,vect_k = c(20:300),
                             Title_graphic  = "Extreme of L2 after transformation")
```

We obtain a heavy-tail variable. We can now apply the results in regular variations to simulate new extreme time series. 
## 4) Selection of extreme time series
```{r}
# L<-500
# Analyse_extreme_proj(base_RV = Frechet,nb_scores = 3,
#       stand_ = TRUE,name_variable ="Toy",L=L,NB_year=37,M1=200,M2=300)

```

```{r}
L2_norm<-apply(X = Frechet,MARGIN = 1,FUN = calcul_norme_L2)
Threshold_l<-20
Min_u<-quantile(L2_norm,0.75)
Max_u<-quantile(L2_norm,0.98)
NB_TH_for_graphs<-200
POT::tcplot(L2_norm,ask = FALSE,u.range = c(Min_u,Max_u),
            which = 1,nt =NB_TH_for_graphs)
abline(v=Threshold_l,col="red")
obj_TC<-POT::tcplot(L2_norm,ask = FALSE,u.range = c(Min_u,Max_u),
            which = 2,nt = NB_TH_for_graphs)
abline(v=Threshold_l,col="red")
abline(h=1.10,col="green")
abline(h=1,col="orange")
```




```{r}
POT::mrlplot(data = L2_norm,ask = FALSE,u.range = c(Min_u,Max_u),
            nt = NB_TH_for_graphs)
abline(v=Threshold_l,col="red")
```



```{r}
Index_extremes<-which(L2_norm>Threshold_l)
Exts_Pto<-Frechet[Index_extremes,]
Inds_drawn<-sample(c(1:nrow(Exts_Pto)),size = Nshown)
Theta<-t(t(Exts_Pto)%*%diag(L2_norm[Index_extremes]^(-1)))
matplot(t(Theta),main="Shape found",type="l")
```

## 5) Approach the angle of the transformed time series
```{r}
STD_PCA<-TRUE
### As we standardise the time series, we save its mean and its standard deviation 
### at each time.
esp_time<-colMeans(Theta)
sd_time<-apply(X = Theta,MARGIN = 2,FUN = sd)
### PCA decomposition
PCA_analysis<-FactoMineR::PCA(X = Theta,scale.unit =STD_PCA ,graph = FALSE)

### Eigenvectors.
F_eigen<-PCA_analysis$svd$V
val_lambda<-PCA_analysis$eig[,1]
vect_diff<-cumsum(c(0,val_lambda))/sum(val_lambda)
vecteur_propvarexp<-1-vect_diff

### We choose the number of dimensions to keep with the proporion of unexplained inertia.
L<-length(vecteur_propvarexp)-1
plot(c(0:L),vecteur_propvarexp,
       main="Evolution of axis contribution with standardisation",
       type="o",ylab="Proportion of unexplained inertia", 
       xlab="Number of eigenvectors J")
```
```{r}
val_lambda
```


We keep the first $J$ eigenvectors. Thus, we reduce the problem dimension as we only approach the law of the first two coordinates.
```{r}
### Eigenvectors obtained. 
Min_f<-min(apply(X = F_eigen,MARGIN = 1,FUN = min))
Max_f<-max(apply(X = F_eigen,MARGIN = 1,FUN = max))
plot(c(1:37),F_eigen[,1],type="l",ylim=c(Min_f,Max_f),xlab="Time",
     ylab=expression(nu[j]))
lines(c(1:37),F_eigen[,2],col="red")
lines(c(1:37),F_eigen[,3],col="orange")
lines(c(1:37),F_eigen[,4],col="green")
```
### Copula theory to approach the law of coordinates
```{r}
### Coordinates in the PCA basis
J<-2
Scores_<-PCA_analysis$ind$coord[,c(1:J)]
Matrix_C<-function_Structure_Matrice(NB_dim = J)
percent_variance_Theta<-PCA_analysis$eig[c(1:J),2]

```


```{r}
### Uniform margins
Unifs<-VineCopula::pobs(Scores_)
### Choose the model to use (AIC criterion)
if(J>2){
  chosen_Model<-VineCopula::RVineCopSelect(data=Unifs,Matrix = Matrix_C)
}
if(J<=2)
{
  chosen_Model<-BiCopSelect(Unifs[,1],Unifs[,2])
}
Sumy<-summary(chosen_Model)

```

We can plot the isodensity curves we obtain for each couple of coordinates in our simulations and in our data. 

```{r}
N_for_test<-nrow(Unifs)
if(J>2){
  choice_couple<-1
  coords_ij<-Sumy$edge[choice_couple]
  i<-as.numeric(substr(coords_ij,start = 1,stop = 1))
  j<-as.numeric(substr(coords_ij,start = 3,stop = 3))
  Simul_for_test<-VineCopula::RVineSim(N=N_for_test,chosen_Model)[,c(i,j)]
}
if(J==2)
{
  i<-1
  j<-2
  Simul_for_test<-BiCopSim(N=N_for_test,family =chosen_Model$family,par = chosen_Model$par,par2 =chosen_Model$par2,check.pars = TRUE)
}

```

```{r}
expr_i<-function_expression_prop_variance_j(j = i,prop_variance = percent_variance_Theta)
expr_j<-function_expression_prop_variance_j(j = j,prop_variance = percent_variance_Theta)

MIN_y<-min(apply(Simul_for_test,MARGIN = 1,FUN = min),apply(Unifs,MARGIN = 1,FUN = min))
MAX_y<-max(apply(Simul_for_test,MARGIN = 1,FUN = max),apply(Unifs,MARGIN = 1,FUN = max))

colnames(Simul_for_test)<-c("first","second")

data<-cbind.data.frame(Unifs[,i],Unifs[,j])
colnames(data)<-c("first","second")
DATA_ALL<-rbind.data.frame(data,Simul_for_test)
DATA_ALL$origin<-c(rep("data",nrow(data)),rep("simul",nrow(Simul_for_test)))
```

```{r}
chosen_Model
```

```{r}
# ggplot(data=DATA_ALL,aes(x = first, y =second)) +
#   geom_point()+
#   xlim(c(-0.15,1.15))+
#   ylim(c(-0.15,1.15))+
#   geom_density_2d_filled(alpha = 0.5) + facet_wrap(vars(origin))+
#   geom_density_2d(aes(linetype=origin))+
#   scale_linetype_manual(values = c("simul"=2,"data"=1))+
#   ggtitle("Iso-density curves of the simulated and recorded coordinates")+
#   labs(linetype="Legend",fill="Level")+
#    xlab(expr_i)+
#    ylab(expr_j)+
#    theme(axis.title=element_text(size=15))
```
```{r}
contour(chosen_Model)
points(Scores_,col="blue")
```
```{r}
### KDE copula
# require(kdecopula)
# Copkde<-kdecop(udata = Unifs)
# contour(Copkde,margins = "unif")
# points(Unifs,col="blue")
```

As we are able to approach the law of angle with the PCA, we now use the results in regular variations to simulate extreme time series with heavy-tail margins. 


#(II) Simulation of extreme time series

##(1) Simulate new angles
```{r}
#### the chosen number of simulated extreme time series. 
Nsimul<-2000

Nfound<-0
New_obs_P_acp<-matrix(NA,nrow = Nsimul,ncol = ncol(Theta))
while (Nfound<Nsimul){
  Nb<-Nsimul-Nfound
  if(J<=2){
    #param
  Realisations_copules<-BiCopSim(N=Nb,family =chosen_Model$family,par = chosen_Model$par,par2 =chosen_Model$par2,check.pars = TRUE)
  #not param
  # Realisations_copules<-kdecopula::rkdecop(n= Nsimul,obj = Copkde)
  }
  if(J>2){
    #param
    Realisations_copules<-VineCopula::RVineSim(N=Nb,RVM = chosen_Model)
    #not param
    # Realisations_copules<-kdecopula::rkdecop(n=Nsimul,obj = Copkde)
  }
  Matrice_coords_ALL<-matrix(NA,ncol = J,nrow = Nb)
  for(l in c(1:J)){
    if(Nb!=1){
      Matrice_coords_ALL[,l]<-quantile(Scores_[,l],probs=Realisations_copules[,l])
    }
    else{
      Matrice_coords_ALL[,l]<-quantile(Scores_[,l],probs=Realisations_copules[l])
    }
    
  }
  ### Simulate new angles with the PCA decomposition
  Shape_Theta<-function_reconstitution_trajectory_std(Vector_coords=Matrice_coords_ALL,Base_functions_p = F_eigen,NB_dim = J, mu_t = esp_time,
    sd_t = sd_time,BOOL_STD=STD_PCA)
  
  ### Impose the condition on the L2 norm of the angle
  L2_simul<-apply(X =Shape_Theta,MARGIN = 1,FUN = calcul_norme_L2)
  Shape_Theta<-t(t(Shape_Theta)%*%diag(L2_simul^(-1),ncol = Nb))
  
  ### Simulate a new radius with a Pareto law. 
  valeurs_unif<-runif(n = Nb)
  reals_P_std<-sapply(valeurs_unif,Generation_Pareto_std)
  Scale_Pareto<-reals_P_std*as.numeric(Threshold_l)
  New_P_candidates<-t(t(Shape_Theta)%*%diag(Scale_Pareto,ncol = Nb))
  seuil_marg<-log(10^5)^(-1)
  Inds_Pareto_verified<-apply(X = New_P_candidates-seuil_marg,MARGIN = 1,FUN =Function_margin_check)
  
  ### We check that we are above 0. 
  Selected<-which(Inds_Pareto_verified==TRUE)
  ### Apply the selection
  if(length(Selected)>0){
    Sub<-New_P_candidates[Selected,]
    if(is.null(nrow(Sub))){
      Sub<-matrix(Sub,ncol=ncol(New_obs_P_acp),nrow=1)
    }
    ### fill the empty dataset with the convenient time series. 
    beg_indexes<-Nfound+1
    end<-Nfound+length(Selected)
    New_obs_P_acp[c(beg_indexes:end),]<-Sub
    Nfound<-end
  }
}

```

```{r}
Shape_Theta<-t(t(New_obs_P_acp)%*%diag(apply(MARGIN = 1,FUN = calcul_norme_L2,X=New_obs_P_acp )^(-1)))
```


```{r}
par(mfrow=c(1,2))
matplot(t(Exts_Pto[Inds_drawn,]),type="l",ylab="value",
        main="Extreme observations",xlab="Time")
matplot(t(New_obs_P_acp[Inds_drawn,]),
        type="l",ylab="value",main="Simulated time series",
        xlab="Time")
par(mfrow=c(1,1))
```
```{r}
par(mfrow=c(1,2))
matplot(t(Theta[Inds_drawn,]),type="l",ylab="value", main="Angle of extreme observations",xlab="Time")
matplot(t(Shape_Theta[Inds_drawn,]),type="l",ylab="value",main="Simulations of the angle",xlab="Time")
par(mfrow=c(1,1))
```

```{r}
### Observe the Angle we obtain.
### After normalising our simulations. 
Shape_simulated<-t(t(New_obs_P_acp)%*%diag(apply(X=New_obs_P_acp,MARGIN = 1,FUN = calcul_norme_L2)^(-1)))

par(mfrow=c(1,2))
matplot(t(Theta),type="l",ylab="value",
        main="Angle of extreme observations",xlab="Time")
matplot(t(Shape_Theta),
        type="l",ylab="value",main="Simulations of the angle",
        xlab="Time")
par(mfrow=c(1,1))
```



## 2) Applying the reverse marginal transformation
```{r}
INDICES_ACP<-1:nrow(New_obs_P_acp)
### Pareto margins.
Unifs_P<-exp(-New_obs_P_acp^(-1))
New_obs_P_acp<-(1-Unifs_P)^(-1)
NO_ACP<-lapply(INDICES_ACP,FUN = fnct_select_colonne,df=New_obs_P_acp)
Variables_reconversion_PCA<-lapply(NO_ACP,function_reconversion_Pareto,K=K_dens,list_evt=L_EVT)
Variables_reconversion_PCA<-t(cbind.data.frame(Variables_reconversion_PCA))
```

As we simulate new extreme residuals, we need to choose an initialisation. 
## (3) Choice of the previous time series
```{r}
SW<-20
l_X_eve<-apply(X_variable,MARGIN = 1,FUN = calcul_norme_L2)
L<-length(l_X_eve)-1
l_Epsilon_data<-apply(Residuals_AR,MARGIN = 1,
                      FUN = calcul_norme_L2)[2:length(l_X_eve)]
l_Epsilon_simul<-apply(X = Variables_reconversion_PCA,MARGIN = 1,
                       FUN = calcul_norme_L2)
Indexes<-as.numeric(sapply(l_Epsilon_simul ,FUN = Sample_window,x_window = l_Epsilon_data,y = l_X_eve[1:L], size_window =SW ))
```

```{r}
obs_taken<-X_variable[Indexes,]
l_taken<-apply(X = obs_taken,MARGIN = 1,
               FUN = calcul_norme_L2)
### Use the AR formula at each time. 
Sim_X<-matrix(NA,nrow =nrow(Variables_reconversion_PCA),
              ncol=ncol(Variables_reconversion_PCA))
for(j in c(1:nrow(Variables_reconversion_PCA))){
  obs_EVE<-obs_taken[j,]
  Prediction<-as.numeric(sapply(X = c(1:ncol(X_variable)),FUN = Function_AR_p, obs_eve=obs_EVE,epsilon_t_plus1=unlist(Variables_reconversion_PCA[j,]), Modele_ar_par_t = Df_reg_ar))
  Sim_X[j,]<-Prediction
}
```


```{r}
X_exts<-X_variable[Index_extremes,]
Inds_shown<-sample(x =c(1:length(Index_extremes)),size = Nshown)
par(mfrow=c(1,2))
matplot(t(Sim_X[Inds_drawn,]),type="l",xlab="Time")
matplot(t(X_exts[Inds_shown,]),type="l",xlab="Time")
mtext("Comparing simulated time series and extreme observations",
      outer=TRUE,line=-3)

```

```{r}
par(mfrow=c(1,2))
matplot(t(Sim_X[1:20,]),type="l")
matplot(t(X_exts[1:20,]),type="l",xlab="Time")
par(mfrow=c(1,1))
```


```{r}
par(mfrow=c(1,2))
L2_simul<-apply(Sim_X,FUN = calcul_norme_L2,
                                     MARGIN=1)
L2_exts<-apply( X=X_exts,MARGIN = 1,FUN = calcul_norme_L2)
Inds_obs<-which(L2_exts>0)
Inds_sim<-which(L2_simul>0)
Angle_simul<-t(t(Sim_X[Inds_sim,])%*%diag(L2_simul[Inds_sim]^(-1)))
Angle_exts<-t(t(X_exts[Inds_obs,])%*%diag(L2_exts[Inds_obs]^(-1)))

matplot(t(Angle_simul),type="l")
matplot(t(Angle_exts),type="l")
mtext("Comparing simulated time series and extreme observations",
      outer=TRUE,line=-3)
```

#(III) Consistency of our simulations. 
##(1) Quantiles value
```{r}
N_rep<-500
B<-nrow(X_exts)
N_rep<-500
lq<-c(0.05,0.50,0.95,0.975)
Tend<-replicate(n =N_rep,expr = Resamples_tendencies(B = B,extremes_indus= X_exts,list_Q =lq))
```

```{r}
estimator_95_simul<-apply(Sim_X,MARGIN =2,                 function(x){return(as.numeric(quantile(x,0.95)))})
estimator_05_simul<-apply(Sim_X,MARGIN =2,    function(x){return(as.numeric(quantile(x,0.05)))})
estimator_50_simul<-apply(Sim_X,MARGIN =2, function(x){return(as.numeric(quantile(x,0.50)))})
estimator_975_simul<-apply(Sim_X,MARGIN =2, function(x){return(as.numeric(quantile(x,0.975)))})
  

### data estimator
estimator_05<-apply(X_exts,MARGIN =2, function(x){return(as.numeric(quantile(x,0.05)))})
estimator_95<-apply(X_exts,MARGIN =2, function(x){return(as.numeric(quantile(x,0.95)))})
estimator_975<-apply(X_exts,MARGIN =2, function(x){return(as.numeric(quantile(x,0.975)))})
estimator_05<-apply(X_exts,MARGIN =2, function(x){return(as.numeric(quantile(x,0.05)))})
estimator_50<-apply(X_exts,MARGIN =2, function(x){return(as.numeric(quantile(x,0.50)))})
  
```


```{r}
bound_minus05<-apply(Tend[1,,],MARGIN = 1,
                     FUN =function(x){return(quantile(x,0.025))})
bound_plus05<-apply(Tend[1,,],MARGIN = 1,
                    FUN = function(x){return(quantile(x,0.975))})

##########
bound_minus50<-apply(Tend[2,,],MARGIN = 1,
                     FUN = function(x){return(quantile(x,0.025))})
bound_plus50<-apply(Tend[2,,],MARGIN = 1,
                    FUN = function(x){return(quantile(x,0.975))})

########
bound_minus95<-apply(Tend[3,,],MARGIN = 1,
                     FUN = function(x){return(quantile(x,0.025))})

bound_plus95<-apply(Tend[3,,],MARGIN = 1,
                    FUN = function(x){return(quantile(x,0.975))})
######
bound_minus975<-apply(Tend[4,,],MARGIN = 1,
                      FUN = function(x){return(quantile(x,0.025))})

bound_plus975<-apply(Tend[4,,],MARGIN = 1,
                     FUN = function(x){return(quantile(x,0.975))})
```

```{r}
df<-cbind.data.frame(c(estimator_05,estimator_50,estimator_95,estimator_975),
                       c(bound_minus05,bound_minus50,
                         bound_minus95,bound_minus975),
                       c(bound_plus05,bound_plus50,
                         bound_plus95,bound_plus975))
colnames(df)<-c("estimator","bound_minus","bound_plus")

cols_<-c("data"="blue","simul"="orange","confidence_band"="darkblue")
df$estimator_simul<-c(estimator_05_simul,estimator_50_simul
                         ,estimator_95_simul,estimator_975_simul)
df$Time<-rep(c(1:ncol(X_exts)),4)
df$percent<-c(rep("Q05",ncol(X_exts)),
                 rep("Q50",ncol(X_exts)),
                 rep("Q95",ncol(X_exts)), 
                 rep("Q975",ncol(X_exts)))
GG_percent<-ggplot(data=df,aes(x=Time,y=estimator,group=interaction(percent),col="data"))+
    facet_wrap(~percent,scales="free_y")+
    geom_line()+
    geom_point()+
    geom_ribbon(mapping = aes(ymin=bound_minus,ymax=bound_plus,col="confidence_band"),alpha=0.15,
                fill="grey", linetype = "dashed")+
    geom_line(aes(x=Time,y=estimator_simul,col="simul"),linetype=2)+
    theme(axis.title=element_text(size=15))+
    ylab("value of the variable")+
    scale_color_manual(values=cols_)+
    xlab("Time")+
    labs(col="Legend")
GG_percent
```

##(2) Functional decomposition of simulated and recorded time series
```{r}

PCA_plan<-FactoMineR::PCA(X = Angle_exts,scale.unit = TRUE,graph=FALSE)
Mu<-colMeans(Angle_exts)
SD<-apply(Angle_exts,MARGIN = 2,FUN = sd)
F_eigen_for_analysis<-PCA_plan$svd$V[,c(1:2)]
Coordinates_obs<-PCA_plan$ind$coord[,c(1:2)]
percent_variance<-PCA_plan$eig[c(1:J),2]
### Standardise the simulations
STD_Simul<-scale(Angle_simul,center = Mu,scale = SD)
Coordinates_simul<-STD_Simul%*%(F_eigen_for_analysis)
```

```{r}
colnames(Coordinates_simul)<-c("Score_1","Score_2")
colnames(Coordinates_obs)<-c("Score_1","Score_2")
df_combo<-rbind.data.frame(Coordinates_obs,Coordinates_simul)
df_combo$type<-c(rep("data",nrow(Coordinates_obs)),rep("simul",nrow(Coordinates_simul)))
GG0<-ggplot(data=Coordinates_obs,aes(x=Score_1,y=Score_2,col="data"))+
  geom_point()+
  geom_point(data=Coordinates_simul,aes(x=Score_1,y=Score_2,col="simul"),
             alpha=0.5,
             pch=20)+
  geom_xsidedensity(data=df_combo,aes(fill=type), alpha = 0.5)+
  geom_ysidedensity(data=df_combo,aes(fill=type), alpha = 0.5)+
  ggtitle("PCA coordinates of simulated and observed time series")+
  xlab(paste0("First dimension (",round(percent_variance[1],1),"% of the variance)"))+
  ylab(paste0("Second dimension (",round(percent_variance[2],1),"% of the variance)"))+
  labs(col="Legend")+
  scale_color_manual(values=cols_)+
  scale_fill_manual(values=cols_)+
  guides(fill="none")+
  theme(axis.title=element_text(size=15))
GG0
```
We can use statistic tests to compare the distribution of the simulated coordinates and the one of the observed coordinates.
```{r}
ks.test(Coordinates_obs[,1],Coordinates_simul[,1])$p.value
ks.test(Coordinates_obs[,2],Coordinates_simul[,2])$p.value
```
The coordinates are quite different. 
We can now see if some algorithms are able to distinguish our simulations from extreme observations. We use several entries such as the time series, its norm and its pattern represented by its angle. 
## (3) Extreme values 
We simulate new extreme values but we want that they stay coherent with the time series we study. One way to do so is to use the theoretical law of exceedances at each time.
```{r}
periods_years<-c(2,5,10,20,50,80,
                 100,120,200,
                 250,300,500,
                 800)
limite_sup<-3*37
periods_years<-periods_years[which(periods_years<limite_sup)]
p_u<-rep(0.05,ncol(X_exts))
Nom_v<-"dummy"
Y_graph_lim<-c(3,12)
for(t in c(13,19,25,31)){
  lag<-t-19
  comment<-ifelse(lag<0,paste0(abs(round(lag/6)),"h before"),
                  ifelse(lag==0," at",paste0(round(lag/6),"h after")))
  series_simul<-Sim_X[,t]
  series_t<-X_variable[,t]
  seuil<-quantile(series_t,1-p_u[t])
  
  series_ext<-subset(series_t,series_t>seuil)
  NB_years_total<-37
  npy<-length(series_t)/NB_years_total
  p_proba<-1-(npy*periods_years*p_u[t])^(-1)
  RL_ggplot_cond_ext(series = series_t,seuil = seuil,
                     period_years = periods_years,
                     NPY = round(npy),
                     plus_simul = TRUE,
                     series_simul = series_simul, 
                     titre = paste0("Return level (ML) ",comment, " the tidal peak for ",Nom_graph), 
                     nom_variable = Nom_v, 
                     cols_ggplot = cols_, 
                     unit_used = "m",
                     ylim_opt = Y_graph_lim, 
                     alpha=0.05,
                     Individus_exts=Index_extremes)

}
```

## (4) Correlation of extreme values
```{r}
Matrice_couples<-list()
L<-ncol(X_exts)
z<-1
vecteur_distances<-c()
for(j in 1:L){
    #condition imposée sur le deuxième temps.
    valeurs_t_plus_h<-j:L
    for (i in valeurs_t_plus_h){
      Matrice_couples[[z]]<-c(i,j)
      vecteur_distances<-c(vecteur_distances,abs(i-j))
      z<-z+1
    }
}
q_chosen<-0.90
Tau<-sapply(c(1:37),function(x,q,t){
  return(as.numeric(quantile(x[,t],q)))},q=q_chosen,
  x=X_exts)
```


```{r}
Extremogram_simulations<-empirical_extremogram(Matrix_couples =Matrice_couples,Tau = Tau,inds_select= Sim_X)
Extremogram_data<-empirical_extremogram(Matrix_couples=Matrice_couples,inds_select = X_exts,Tau = Tau)
N_rep<-500
B<-nrow(X_exts)
Result<-replicate(n =N_rep,expr = fnct_estim_extremo_resample(B = B,inds_ext = X_exts,Tau = Tau,Matrix_couples =  Matrice_couples,vector_distances=vecteur_distances))
```

```{r}
fnct_quantile<-function(x,q){
    M<-apply(X = x,MARGIN = 2,FUN = function(X){return(as.numeric(quantile(X,q)))})
    return(M)
  }
Q05<-fnct_quantile(x = t(Result),q=0.025)
Q95<-fnct_quantile(x = t(Result),q=0.975)
df<-cbind.data.frame(Extremogram_simulations,Extremogram_data,vecteur_distances)
colnames(df)<-c("simulation_value","data_value","delta")

result_delta<-df %>% group_by(delta) %>% summarise(val_data=mean(data_value),                                    val_simul=mean(simulation_value))
result_delta$bound_inf<-Q05
result_delta$bound_sup<-Q95
extremo_<-as.data.frame(result_delta[,c("val_data","val_simul","bound_inf","bound_sup")])
extremo_$Time<-c(1:nrow(extremo_))
```

```{r}
GG_extremo<-ggplot(extremo_,aes(x=Time,y=val_data,col="data"))+
  geom_line()+
  geom_point()+
  geom_line(aes(y=val_simul,col="simul"))+
  geom_point(aes(y=val_simul,col="simul"),pch=2)+
  geom_ribbon(mapping = aes(ymin=bound_inf,ymax=bound_sup,col="confidence_band"),alpha=0.15,
              fill="grey", linetype = "dashed")+
  scale_color_manual(values=cols_)+
  theme(axis.title=element_text(size=15))+
  labs(col="Legend")+
  xlab("Lag h")+
  ylab("value")
GG_extremo
```


```{r}
TauF<-sapply(c(1:37),function(x,q,t){
  return(as.numeric(quantile(x[,t],q)))},q=q_chosen,
  x=Exts_Pto)
Extremogram_simulations_F<-empirical_extremogram(Matrix_couples =Matrice_couples,Tau =TauF,inds_select= New_obs_P_acp)
Extremogram_data_F<-empirical_extremogram(Matrix_couples=Matrice_couples,inds_select = Exts_Pto,Tau = TauF)
df_Frech<-cbind.data.frame(Extremogram_simulations_F,Extremogram_data_F,vecteur_distances)
colnames(df_Frech)<-c("simulation_value","data_value","delta")

result_deltaFrech<-df %>% group_by(delta) %>% summarise(val_data=mean(data_value),                                    val_simul=mean(simulation_value))

```

```{r}
plot(c(0:36),result_deltaFrech$val_data)
lines(c(0:36),result_deltaFrech$val_simul,
      col="red")
```

The simulator of the simulated time series is above what we have in the observations.

## (4) Identifying simulated time series with machine learning algorithms

```{r}
par(mfrow=c(1,1))
### Hyper-parameters of our machine learning algorithms.
H_Params<-c("radial","logit",500)

### Parameters of our experience. 
NB_times<-100
ALPHA_PROP<-0.10
typeS<-"simple"
result<-matrix(NA,ncol=3,nrow=3)
```

```{r}
det(t(X_exts)%*%X_exts)
```


```{r}
print(qr(X_exts)$rank)
print(qr(Sim_X)$rank)
```

```{r}
# Use the time series as input
Result_X<-Running_perfs_ML(Base_simul =Sim_X,
                 Base_data = X_exts,
                 hyp_param = H_Params,
                 NB_times = NB_times,
                 alpha_prop = ALPHA_PROP,
                 K = K,
                 type_sampling = typeS,
                 title_ROC = "ROC curves",
                 NB_shown_ROC = 20)
result[1,]<-Result_X$Accuracy$qu_accuracy
```

```{r}
### Use the L2 norm as input 
Result_l_f<-Running_perfs_ML(Base_simul =L2_simul,Base_data =L2_exts,
 hyp_param = H_Params,NB_times = NB_times,
 alpha_prop = ALPHA_PROP, K = K,
type_sampling = typeS, title_ROC = "ROC curves",
 NB_shown_ROC = 20)
result[2,]<-Result_l_f$Accuracy$qu_accuracy
```


```{r}
### Use the angle as input. 
Result_A_f<-Running_perfs_ML(Base_simul =Angle_simul,
                 Base_data = Angle_exts,
                 hyp_param = H_Params,
                 NB_times = NB_times,
                 alpha_prop = ALPHA_PROP,
                 K = K,
                 type_sampling = typeS,
                 title_ROC = "ROC curves",
                 NB_shown_ROC = 50)
result[3,]<-Result_A_f$Accuracy$qu_accuracy
```

### Summary of our resuts
```{r}
result
```
```{r}
n<-200
d<-ncol(X_exts)
Noise_Gauss<-MASS::mvrnorm(n = n,mu = rep(0,d),
                           Sigma = diag(d))
simul_train<-Sim_X[1:n,]
real_train<-X_exts[1:n,]

```

```{r}

realisations<-rbind(real_train,simul_train)

mean_realisations<-apply(X = realisations,MARGIN = 2,FUN=mean)
sd_realisations<-apply(X = realisations,MARGIN = 2, 
                         FUN =sd)
realisations<-as.data.frame(scale(as.matrix(realisations),
                                  ))
summary(apply(realisations,MARGIN = 2,FUN = mean))
summary(apply(realisations,MARGIN = 2,FUN = sd))
  # Variable cible : indicatrice simulation ---------------------------------
# y=1 pour les simulations et y=0 pour les observations
realisations$y<-c(rep(0,nrow(real_train)),rep(1,nrow(simul_train)))

```


```{r}
  # Echantillon d'apprentissage ---------------------------------------------
  
realisations$y<-as.factor(realisations$y)
  
  # Shuffle -----------------------------------------------------------------
realisations<-realisations[sample(nrow(realisations)),]

```


```{r}
results<-glm(y~., family = binomial(),data = realisations)
table(round(results$fitted.values),realisations$y)
```
```{r}
LDA_model<-MASS::lda(y~.,realisations)
```
```{r}
Y_pred<-predict(LDA_model,newdata = realisations)$class
table(Y_pred,realisations$y)
```
```{r}
Model_binom<-glmnet::glmnet(realisations[,c(1:37)],realisations[,38], 
               family = "binomial")
#x<-as.matrix(realisations)[,c(1:37)]
x<-realisations[c(1:37)]

```

```{r}
x<-as.matrix(x)
typeof(x)
```


```{r}
Y_pred<-predict(Model_binom,x,s=0.01,
                type = "response")
table(round(Y_pred),realisations$y)
```



