---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r}
rm(list=ls())
set.seed(133)
par(mfrow=c(1,1))
source("fonctions/fonctions.R")
require(reshape2)
require(ggplot2)
require(gridExtra)
require(grid)
require(dplyr)
require(extRemes)
require(ggside)
require(scales)
require(dplyr)
require(corrplot)
```

```{r}
rm(list=ls())
set.seed(133)
par(mfrow=c(1,1))
source("fonctions/fonctions.R")
require(reshape2)
require(ggplot2)
require(gridExtra)
require(grid)
require(dplyr)
require(extRemes)
require(ggside)
require(scales)
```

```{r}
cols_<-c("data"="blue","simulations"="orange","confidence_band"="darkblue")
```

```{r}
Simulate_extreme_from_resid<-function(nb_decal,Sample_taken,
                                        obs,Mod_AR,j,Simul_epsi){
    index<-sort(c(Sample_taken[j])+c(-nb_decal:0),decreasing = TRUE)
    individu_taken<-obs[index,]
    Prediction<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p,
                                  obs_eve=individu_taken,
                                  epsilon_t_plus1=unlist(Simul_epsi[j,]),
                                  model_ar_par_t = Mod_AR))
    return(Prediction)
}
```


```{r}
name_variable<-"Surcote"
name_for_file<-ifelse(name_variable=="Surcote","S",no = name_variable)
fich_reg1<-paste0("residuals/",name_for_file,"_Reg_AR1.csv")

file_Simulation_Residuals<-paste0("residuals/Simulations_",name_variable,"_avec_std.csv")
file_idx_extremes<-paste0("inds_extremes_data_",name_variable,".csv")
dates_Johanna<-c("10/03/2008","08/03/2008", 
                 "09/03/2008")
```

```{r}
name_search<-paste0("data_detrend_Winter/",name_variable,"_detrend.csv")

# Observe the norm of residuals -------------------------------------------
Residuals<-read.csv(file = "residuals/S_residuals.csv")[,2:38]
colnames(Residuals)<-c(1:ncol(Residuals))

NL2_Residuals<-apply(X = Residuals,MARGIN = 1,FUN = calcul_norme_L2)
obs<-read.csv(name_search)[,2:38]
colnames(obs)<-c(1:37)

# Import of extreme individuals in the base ------------------------------
file_idx_extremes<-paste0("residuals/",file_idx_extremes)
Inds_extremes_L2<-read.csv(file_idx_extremes)[,2]
Individuals_extremes_base<-obs[Inds_extremes_L2,]
rownames(Individuals_extremes_base)<-c(1:nrow(Individuals_extremes_base))
NL2_Residuals_exts<-NL2_Residuals[Inds_extremes_L2]

# Import dates ------------------------------------------------------------
Taken_dates<-read.csv(file="data_detrend_Winter/dates.csv")[,2]
Taken_dates_extremes<-Taken_dates[Inds_extremes_L2]
mini_date<-substr(Taken_dates_extremes,1,10)
index_Joh<-which(mini_date%in%dates_Johanna)
date_J_found<-Taken_dates_extremes[index_Joh]
# On prend la derniÃ¨re obs ------------------------------------------------
Simulations_epsi<-read.csv(file=file_Simulation_Residuals)[,2:38]
NL2_epsi_simul<-apply(X=Simulations_epsi,
                      MARGIN=1,FUN = calcul_norme_L2)
par(mfrow=c(1,2))
colnames(Simulations_epsi)<-c(1:37)
matplot(t(Simulations_epsi),type="l")
matplot(t(Residuals[Inds_extremes_L2,]),type="l")
par(mfrow=c(1,1))
```


```{r}
# Eventual relation between  epsilon_t and Residuals -------------------------------------------
number_obs<-nrow(obs)-1
df_result<-matrix(NA,nrow = 37,ncol = 4)
DF_EV_resid<-read.csv(file=paste0("Work_RVariations/EVA_",name_variable,"_Residuals.csv"))[,2:5]
vect_threshold_GPD<-DF_EV_resid$seuil_t

# Correlation_t=19 --------------------------------------------------------
val_peak<-obs[1:number_obs,19]
r_peak<-Residuals[c(2:nrow(obs)),19]

for(t in c(1:ncol(obs))){

  # subset of extreme/Mann Whitney ------------------------------------------
  threshd_t<-vect_threshold_GPD[t]
  series_r<-Residuals[c(2:nrow(obs)),t]
  eve_r<-obs[1:number_obs,t]
  indexs_ext<-which(series_r>threshd_t)
  index_non_ext<-which(series_r<=threshd_t)
  subset1<-eve_r[indexs_ext]
  subset2<-eve_r[index_non_ext]

  # Correlation test --------------------------------------------------------

  Pearson_t<-cor.test(x = series_r,y=eve_r,method="pearson")$p.value
  Spearman_t<-cor.test(x = series_r,y=eve_r,method="spearman")$p.value
  Kendall_t<-cor.test(x = series_r,y=eve_r,method="kendall")$p.value
  MaWhitney_U<-wilcox.test(subset1,subset2)$p.value
  df_result[t,]<-c(Pearson_t, Spearman_t,
                     Kendall_t,MaWhitney_U)
}

# model AR ---------------------------------------------------------------
Mod_AR<-read.csv(file=fich_reg1)
Fin<-ncol(Mod_AR)
Mod_AR<-Mod_AR[,c(2:Fin)]

M<-nrow(Simulations_epsi)
Y_Mat<-matrix(NA,nrow = M,ncol = 37)
sampling_YO<-function(niv_simul,echs_N2){
  return(which.min(abs(niv_simul-echs_N2)))
}
# Work with non extreme individuals -------------------------------------------
indexs_non_ext<-which(!c(1:nrow(obs))%in%Inds_extremes_L2==TRUE)

NL2<-apply(X = obs,MARGIN = 1,FUN = calcul_norme_L2)
inds_non_nuls<-which(NL2_Residuals>0)
plot(NL2_Residuals[inds_non_nuls],NL2[inds_non_nuls],
     main="Relation of the two models", 
     xlab=expression("Epsilon Norm"[M]),ylab=expression("Y Norm"[M]))
```

```{r}
y<-NL2[inds_non_nuls]
x<-NL2_Residuals[inds_non_nuls]
model_lin_epsi_Y<-lm(y~x)
JBtest<-tseries::jarque.bera.test(model_lin_epsi_Y$residuals)
threshd_y<-quantile(y,0.75)
threshd_x<-quantile(x,0.75)
number_obs<-length(NL2)-1
NL2_eve<-NL2[1:number_obs]
filtre<-min(NL2[Inds_extremes_L2])

prob_non_exceedance<-mean(as.numeric(NL2<=filtre))
print("Return period")
npy<-length(NL2)/37
periode_retour<-(npy*(1-prob_non_exceedance))^(-1)
indexs_non_ext<-which(NL2<filtre)
par(mfrow=c(1,1))
N_XO<-c()
df_relation_inbase<-cbind.data.frame(NL2_Residuals[2:length(NL2_Residuals)],
                                     NL2_eve)

colnames(df_relation_inbase)<-c("epsi","L2_indu_taken")
```


```{r}
inds_chosen<-which((df_relation_inbase$epsi>0))
df_subset<-df_relation_inbase[inds_chosen,]
## (3) Choice of the previous time series
ech_window<-sapply(X =NL2_epsi_simul,Sample_window,
         y=NL2_eve,x_window=NL2_Residuals[2:length(NL2_Residuals)],size_window=20)
Taken_samples<-ech_window
```

```{r}
L<-nrow(obs)-1
sub_obs<-NL2[1:L]
sub_resid<-NL2_Residuals[2:nrow(obs)]
sub_obs<-sub_obs[which(sub_resid>0)]
sub_resid<-subset(sub_resid,sub_resid>0)
df<-cbind.data.frame(sub_obs,sub_resid)
colnames(df)<-c("obs_passe","resid")
model_choix<-lm(obs_passe~resid,data = df)
predictions_niveaux_passe<-predict(model_choix,data.frame(resid=NL2_epsi_simul))

par(mfrow=c(1,2))
plot(density(NL2[Taken_samples]),main="norme L2 of obs (X0) takenes")
plot(density(NL2[Inds_extremes_L2-1]),main="norme L2 eve of obs extremes")
par(mfrow=c(1,1))
nb_decal<-0
if(ncol(Mod_AR)>2){
  nb_decal<-ncol(Mod_AR)-2
}
Y_Mat<-t(sapply(c(1:M),Simulate_extreme_from_resid,nb_decal = nb_decal,
                     Sample_taken = Taken_samples ,
                     Mod_AR = Mod_AR,Simul_epsi = Simulations_epsi,
                     obs = obs))
Y_Mat<-as.data.frame(Y_Mat)
colnames(Y_Mat)<-c(1:ncol(Y_Mat))
NL2_simulations<-apply(X =  Y_Mat,MARGIN = 1,FUN=calcul_norme_L2)
print("simul vs eve--YO")
test<-Inds_extremes_L2[2]-1
```


```{r}

# Simulation event type Johanna ---------------------------------------
eve_Johann<-obs[Inds_extremes_L2[index_Joh]-1,]
date_eve_J<-Taken_dates[Inds_extremes_L2[index_Joh]-1]
Johann<-obs[Inds_extremes_L2[index_Joh],]
Next_day_extreme<-obs[Inds_extremes_L2[index_Joh]+1,]
M_Prediction_obs_J<-t(sapply(c(1:nrow(Simulations_epsi)),
                            FUN = function(x)
    {vector<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p,
                                                   obs_eve=eve_Johann,epsilon_t_plus1=unlist(Simulations_epsi[x,]),
                                                   model_ar_par_t = Mod_AR))
     return(vector)}))
M_Prediction_consecutive_extreme<-t(sapply(c(1:nrow(Simulations_epsi)),
                           FUN = function(x)
                           {vector<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p,obs_eve=Johann,             epsilon_t_plus1=unlist(Simulations_epsi[x,]),model_ar_par_t = Mod_AR))
                           return(vector)}))

# versus observation normal -----------------------------------------------

X_zero_normal<-obs[Taken_samples[1],]
date_XOnormal<-Taken_dates[Taken_samples[1]]
Next_day_normal<-obs[Taken_samples[1]+1,]

M_Prediction_normal_day<-t(sapply(c(1:nrow(Simulations_epsi)),
         FUN = function(x){vector<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p, obs_eve=X_zero_normal,epsilon_t_plus1=unlist(Simulations_epsi[x,]),
          model_ar_par_t = Mod_AR))
               return(vector)}))

```

```{r}
# Relation eve_XO vs norme epsilon -------------------------------------
NL2_epsi_simul<-apply(X = Simulations_epsi,MARGIN = 1,
                      FUN = calcul_norme_L2)
df_Residuals_vs_eve_simul<-cbind.data.frame(NL2_epsi_simul,
                                            NL2[Taken_samples])
colnames(df_Residuals_vs_eve_simul)<-c("simul_epsi","L2_indu_taken")
df_Residuals_vs_eve_simul$indicatrice<-sapply(Taken_samples,function(x){return(x%in%Inds_extremes_L2)})

  # Simulations -------------------------------------------------------------
compar_pic<-cbind.data.frame(r_peak,val_peak)
colnames(compar_pic)<-c("r_peak","val_peak")
GG_rel_v2<-ggplot(data=df_Residuals_vs_eve_simul,aes(x=simul_epsi,
                                                      y=L2_indu_taken,col="simulations"))+
  geom_point(pch=17)+
  xlab(expression("L2 norm of "~epsilon[M]))+
  ylab(expression("L2 norm of "~tilde(X)[M-1]))+
  geom_point(data = df_relation_inbase,aes(y=L2_indu_taken,
                                           x=epsi,col="data"))+
  scale_color_manual(values=c("simulations"="orange","data"="blue"))+
  labs(col="Legend")+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))

print(GG_rel_v2)
```


```{r}
L_predJ<-list("simulations"=M_Prediction_obs_J,
              "X0"=Eve_Johann, 
              "date_X0"=date_eve_J, 
              "X1"=Johann, 
              "date_Johanna"=date_J_found)
lJ<-list("simulations"=M_Prediction_consecutive_extreme,
         "X0"=Johann, 
         "date_J"=date_J_found,
         "X1"=Next_day_extreme)
lnormal<-list("simulations"=M_Prediction_normal_day,
              "X0"=X_zero_normal, 
              "date_XO"=date_XOnormal,
              "X1"=Next_day_normal)
```

```{r}
result_Simul_Obs<-list("simulations_AR"=Y_Mat,"observations_exts_base"=Individuals_extremes_base,"dates_events_extremes"=Taken_dates_extremes,
   "list_Johanna"=lJ,"list_normal"=lnormal,
   "X"=obs,"indexs_X0"=Taken_samples, 
   "indexs_exts"=Inds_extremes_L2,"list_Pred_Johanna"=L_predJ)
```

```{r}

## Data-like time series or consecutive extremes
Obs_ext<-result_Simul_Obs$observations_exts_base
Simul<-result_Simul_Obs$simulations_AR
Inds_drawn<-sample(c(1:nrow(Obs_ext)),size = nrow(Obs_ext)/3,
       replace = FALSE)
```


#(III) Consistency of our simulations. 
##(1) Quantiles value

```{r}
N_rep<-500
B<-nrow(Individuals_extremes_base)
estimator_95<-apply(Individuals_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.95)))})
estimator_975<-apply(Individuals_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.975)))})
estimator_05<-apply(Individuals_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.05)))})
estimator_50<-apply(Individuals_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.50)))})

# estimator simulations --------------------------------------------------
estimator_95_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.95)))})
estimator_05_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.05)))})
estimator_50_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.50)))})
estimator_975_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.975)))})
L_q<-c(0.95,0.975,0.05,0.50)
Tendencies<-replicate(n =N_rep,expr = Resamples_tendencies(B = B,extremes_indus = Individuals_extremes_base,list_Q = L_q))
bound_plus95<-apply(Tendencies[1,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
bound_minus95<-apply(Tendencies[1,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})

bound_plus975<-apply(Tendencies[2,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
bound_minus975<-apply(Tendencies[2,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})

bound_plus05<-apply(Tendencies[3,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
bound_minus05<-apply(Tendencies[3,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})

bound_plus50<-apply(Tendencies[4,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
bound_minus50<-apply(Tendencies[4,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})
```


```{r}
df<-cbind.data.frame(c(estimator_05,estimator_50,estimator_95,estimator_975),
                     c(bound_minus05,bound_minus50,
                       bound_minus95,bound_minus975),
                     c(bound_plus05,bound_plus50,
                       bound_plus95,bound_plus975))
colnames(df)<-c("estimator","bound_minus","bound_plus")

df$estimator_simul<-c(estimator_05_simul,estimator_50_simul
                       ,estimator_95_simul,estimator_975_simul)
df$Time<-rep(c(1:ncol(Individuals_extremes_base)),4)
df$Time<-(df$Time-19)*(1/6)
df$percent<-c(rep("Q05",ncol(Individuals_extremes_base)),
               rep("Q50",ncol(Individuals_extremes_base)),
               rep("Q95",ncol(Individuals_extremes_base)), 
               rep("Q975",ncol(Individuals_extremes_base)))
GG_percent<-ggplot(data=df,aes(x=Time,y=estimator,group=interaction(percent),col="data"))+
  facet_wrap(~percent,scales="free_y")+
  geom_line()+
  geom_point()+
  geom_ribbon(mapping = aes(ymin=bound_minus,ymax=bound_plus,col="confidence_band"),alpha=0.15,
              fill="grey", linetype = "dashed")+
  geom_line(aes(x=Time,y=estimator_simul,col="simulations"),linetype=2)+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))+
  ylab("Surge (m)")+
  scale_color_manual(values=cols_)+
  xlab("Time (hour) with respect to tidal peak")+
  labs(col="Legend")
print(GG_percent)
 
```

##(2) Functional decomposition of simulated and recorded time series

```{r}
NL2_ext<-apply(X = Obs_ext,MARGIN = 1,FUN = calcul_norme_L2)
NL2_simul<-apply(X = Simul,MARGIN = 1,FUN = calcul_norme_L2)
```

```{r}
inds_non_nuls<-which(NL2_simul>0)
Angle_inds<-t(t(Obs_ext)%*%diag(NL2_ext^(-1)))
Angle_simul<-t(t(Simul[inds_non_nuls,])%*%diag(NL2_simul[inds_non_nuls]^(-1)))
PCA_inds<-FactoMineR::PCA(X = Angle_inds,scale.unit = TRUE, 
                          graph = FALSE)
mu_angle<-colMeans(Angle_inds)
percent_variance<-PCA_inds$eig[c(1:2),2]
plot(c(100,PCA_inds$eig[,2]),type="o"
     ,main=paste0("Evolution of the ratio of unexplained inertia for ",ifelse(name_variable=="Surcote","S",name_variable)))
sd_angle<-apply(X =Angle_inds,MARGIN = 2,FUN = sd)
```

```{r}
result<-scale(Angle_simul,center = mu_angle,
                scale = sd_angle)
coords_data<-PCA_inds$ind$coord[,c(1:2)]
conversion_data<-result%*%PCA_inds$svd$V[,c(1:2)]
ks.test(conversion_data[,1],coords_data[,1])
wilcox.test(conversion_data[,1],coords_data[,1])

ks.test(conversion_data[,2],coords_data[,2])
wilcox.test(conversion_data[,2],coords_data[,2])
```

```{r}
df_simul_forme<-cbind.data.frame(conversion_data)
colnames(df_simul_forme)<-sapply(c(1:ncol(df_simul_forme)), 
                               function(x){return(paste0("Score_",x))})

df_data_forme<-as.data.frame(coords_data)
colnames(df_data_forme)<-sapply(c(1:ncol(df_data_forme)), 
                              function(x){return(paste0("Score_",x))})
df_combo<-rbind.data.frame(df_data_forme,df_simul_forme)
df_combo$Legend<-c(rep("data",nrow(df_data_forme)),
                 rep("simulations",nrow(df_simul_forme)))
GG1<-ggplot(data=df_combo,aes(x=Score_1,y=Score_2,colour=Legend,
                              shape=Legend))+
  geom_point(aes(size=Legend))+
  scale_size_manual(values=c("simulations"=0.75,"data"=1.5))+
  geom_xsidedensity(data=df_combo,aes(fill=Legend), alpha = 0.5)+
  geom_ysidedensity(data=df_combo,aes(fill=Legend), alpha = 0.5)+
  xlab(paste0("First dimension (",round(percent_variance[1],1),"% of the variance)"))+
  ylab(paste0("Second dimension (",round(percent_variance[2],1),"% of the variance)"))+
  scale_color_manual(values=cols_)+
  scale_fill_manual(values=cols_)+
  scale_shape_manual(values = c("simulations"=17,"data"=19))+
  guiof(fill="none")+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))
GG1

```

## (3) Correlation of extreme values
```{r}
# Comparison of extremogrammes ------------------------------------------
Matrix_couples<-list()
L<-ncol(Y_Mat)
z<-1
vector_distances<-c()
for(j in 1:L){
  values_t_plus_h<-j:L
  for (i in values_t_plus_h){
    Matrix_couples[[z]]<-c(i,j)
    vector_distances<-c(vector_distances,abs(i-j))
    z<-z+1
  }
}
q_chosen<-0.90
Tau1<-sapply(c(1:37),function(x,q,t){
  return(as.numeric(quantile(x[,t],q)))},q=q_chosen,
  x=Y_Mat)
Tau2<-sapply(c(1:37),function(x,q,t){
  return(as.numeric(quantile(x[,t],q)))},q=q_chosen,
  x=Individuals_extremes_base)
Extremogram_simulations<-empirical_extremogram(Matrix_couples = Matrix_couples,inds_select = Y_Mat,Tau = Tau1)
Extremogram_data<-empirical_extremogram(Matrix_couples = Matrix_couples,inds_select= Individuals_extremes_base,Tau = Tau2)


result<-replicate(n =N_rep,expr = fnct_estim_extremo_resample(B = B,inds_ext  = Individuals_extremes_base,Tau = Tau2,Matrix_couples = Matrix_couples,vector_distances=vector_distances))
```


```{r}
fnct_quantile<-function(x,q){
  M<-apply(X = x,MARGIN = 2,FUN = function(X){return(as.numeric(quantile(X,q)))})
  return(M)
}
Q05<-fnct_quantile(x = t(result),q=0.025)
Q95<-fnct_quantile(x = t(result),q=0.975)
df<-cbind.data.frame(Extremogram_simulations,Extremogram_data,vector_distances)
colnames(df)<-c("value_simulation","value_data","delta")

result_delta<-df %>% group_by(delta) %>% summarise(val_data=mean(value_data),
                                                     val_simul=mean(value_simulation))
result_delta$bound_inf<-Q05
result_delta$bound_sup<-Q95
work_extremo<-as.data.frame(result_delta[,c("val_data","val_simul","bound_inf","bound_sup")])
work_extremo$Time<-c(1:nrow(work_extremo))
GG_extremo<-ggplot(work_extremo,aes(x=Time,y=val_data,col="data"))+
  geom_line()+
  geom_point()+
  geom_line(aes(y=val_simul,col="simulations"))+
  geom_point(aes(y=val_simul,col="simulations"),pch=2)+
  geom_ribbon(mapping = aes(ymin=bound_inf,ymax=bound_sup,col="confidence_band"),alpha=0.15,
              fill="grey", linetype = "dashed")+
  scale_color_manual(values=cols_)+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))+
  labs(col="Legend")+
  xlab("Lag h")+
  ylab("value")

print(GG_extremo)

```

## 3) EVA analysis
```{r}
if(name_variable=="Hs"){
  Y_graph_lim<-c(4,11)
}
if(name_variable=="Surcote"){
  Y_graph_lim<-c(0.13,1)
}
indexs_exts<-read.csv(paste0(file_idx_extremes))[,2]
p_u<-rep(0.05,ncol(Simul))
periods_years<-c(2,5,10,20,50,80,
                 100,120,200,
                 250,300,500,
                 800)
# Define the larger return period according to the number 
# of years that we used, here 37
NB_years<-37
limit_sup<-3*NB_years
periods_years<-periods_years[which(periods_years<limit_sup)]

```

```{r}
for(t in c(13,19,25,31)){
  lag<-t-19
  comment<-ifelse(lag<0,paste0(abs(round(lag/6)),"h before"),
                  ifelse(lag==0," at",paste0(round(lag/6),"h after")))
  series_simul<-Simul[,t]
  series_t<-result_Simul_Obs$X[,t]
  threshd<-quantile(series_t,1-p_u[t])
  
  series_ext<-subset(series_t,series_t>threshd)
  NB_years_total<-37
  npy<-length(series_t)/NB_years_total
  name_graph<-ifelse(name_variable=="Surcote","S",
                    name_variable)
  RL_ggplot_cond_ext(series = series_t,seuil = threshd,
                     period_years = periods_years,
                     NPY = round(npy),
                     plus_simul = TRUE,
                     series_simul = series_simul, 
                     titre = paste0("Return level (ML) ",comment, " the tidal peak for ",name_graph), 
                     name_variable = name_variable, 
                     cols_ggplot = cols_, 
                     unit_used = "m",
                     ylim_opt = Y_graph_lim, 
                     alpha=0.05,
                     Individuals_exts=indexs_exts)
}
```


```{r}
norm<-as.numeric(result_Simul_Obs$list_normal$X0)
simul_Xoest_normal<-melt(t(result_Simul_Obs$list_normal$simulations[Inds_drawn,]))
colnames(simul_Xoest_normal)<-c("time","id","value")
simul_Xoest_normal$id<-as.character(simul_Xoest_normal$id)
simul_Xoest_normal$X_zero<-norm[simul_Xoest_normal$time]
simul_Xoest_normal$time<-(simul_Xoest_normal$time-19)/6
Date_normal<-result_Simul_Obs$list_normal$date_XO
X1_normal<-as.numeric(result_Simul_Obs$list_normal$X1)
GG_normal<-ggplot(data=simul_Xoest_normal,aes(x=time,y=value,
                            group=interaction(id), 
                            col=id))+
  geom_line(alpha=0.7,show.legend=FALSE)+
  xlab(" ")+ylab(" ")+
  geom_line(aes(y=X_zero),col="red", 
            linetype="dotted",size=1.2)+
  annotate("text", x = 0, y = X1_normal[30], 
           label=Date_normal)
```


```{r}
simul_Xoest<-result_Simul_Obs$list_Johanna$simulations[Inds_drawn,]
Joh<-as.numeric(result_Simul_Obs$list_Johanna$X0)
date_X0ext<-result_Simul_Obs$list_Johanna$date_J
simul_Xoest<-melt(t(simul_Xoest))
X1_ext<-as.numeric(result_Simul_Obs$list_Johanna$X1)
colnames(simul_Xoest)<-c("time","id","value")
simul_Xoest$id<-as.character(simul_Xoest$id)
simul_Xoest$X_zero<-Joh[simul_Xoest$time]
simul_Xoest$X_un<-X1_ext[simul_Xoest$time]
simul_Xoest$time<-(simul_Xoest$time-19)/6
GGJoh<-ggplot(data=simul_Xoest,aes(x=time,y=value,
                            group=interaction(id), 
                            col=id))+
  geom_line(alpha=0.7)+
  guiof(col="none")+
  xlab(" ")+ylab(" ")+
  geom_line(aes(y=X_zero),col="red", 
            linetype="dotted",size=1.2)+
  labs(linetype="Date of X0")+
  annotate("text", x = 0, y = X1_ext[30], 
           label=date_X0ext)


```

```{r}
min_grid<-min(min(simul_Xoest$value), min(simul_Xoest_normal$value), 
              min(simul_Xoest$X_zero),min(simul_Xoest_normal$X_zero), 
              X1_normal[30],X1_ext[30])
max_grid<-max(max(simul_Xoest$value), max(simul_Xoest_normal$value), 
              max(simul_Xoest$X_zero),max(simul_Xoest_normal$X_zero),
              X1_normal[30],X1_ext[30])
GG_normal<-GG_normal+ylim(c(min_grid,max_grid))
GGJoh<-GGJoh+ylim(c(min_grid,max_grid))
```

```{r}
name_present<-ifelse(name_variable=="Surcote",yes = "S",no = name_variable)
yleft <- textGrob(paste0(name_present," (m)"),
                  rot=90,
                  gp = gpar(col = "black", fontsize = 15))
Xbottom<-textGrob("Time (hour) with respect to tidal peak",
                  gp = gpar(col = "black", fontsize = 15))

gridExtra::grid.arrange(GGJoh,GG_normal,ncol=2,
                        left=yleft,bottom=Xbottom)
```


##3) Classification two-samples test
```{r}
source("fonctions/fonctions_perfs_ML.R")
racine_ML<-c("results_ML/")
H_Params<-c("radial","logit",500)
NB_times<-100
ALPHA_PROP<-0.10
K<-4
typeS<-"simple"
result<-matrix(NA,ncol=3,nrow=3)

```

```{r}
Lperfs_Y<-Running_perfs_ML(Base_simul = Simul,Base_data = Obs_ext, 
                   hyp_param =H_Params,
                   NB_times=100,alpha_prop=ALPHA_PROP,K = K, 
                   type_sampling = typeS,title_ROC =name_variable,
                   NB_shown_ROC = 20)
result[1,]<-Lperfs_Y$Accuracy$qu_accuracy
```

```{r}
Lperfs_L2<-Running_perfs_ML(Base_simul = NL2_simul,Base_data = NL2_ext, 
                              hyp_param = H_Params, NB_times = NB_times,alpha_prop=ALPHA_PROP, 
                              K=K,type_sampling = typeS
                           ,title_ROC = paste0(name_variable," Cost-f"),NB_shown_ROC = 20)
result[2,]<-Lperfs_L2$Accuracy$qu_accuracy
```

```{r}
Lperfs_A_f<-Running_perfs_ML(Base_simul = Angle_simul,Base_data = Angle_inds, hyp_param =H_Params , NB_times = NB_times,alpha_prop=ALPHA_PROP,K=K,type_sampling = typeS, title_ROC = paste0(name_variable," angle"),NB_shown_ROC = 20)

result[3,]<-Lperfs_A_f$Accuracy$qu_accuracy
```

```{r}
result
```

```{r}
## End of the code
```

