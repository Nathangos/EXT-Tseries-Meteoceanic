---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r}
rm(list=ls())
set.seed(133)
par(mfrow=c(1,1))
print("import functions")
source("fonctions/fonctions.R")
require(reshape2)
require(ggplot2)
require(gridExtra)
require(grid)
require(dplyr)
require(extRemes)
require(ggside)
require(scales)
require(dplyr)
require(corrplot)
```

```{r}
rm(list=ls())
set.seed(133)
par(mfrow=c(1,1))
source("fonctions/fonctions.R")
require(reshape2)
require(ggplot2)
require(gridExtra)
require(grid)
require(dplyr)
require(extRemes)
require(ggside)
require(scales)
```

```{r}
cols_<-c("data"="blue","simulations"="orange","confidence_band"="darkblue")
```

```{r}
Simulate_extreme_from_resid<-function(nb_decal,Sample_taken,
                                        obs,Mod_AR,j,Simul_epsi){
    indice<-sort(c(Sample_taken[j])+c(-nb_decal:0),decreasing = TRUE)
    individu_pris<-obs[indice,]
    Prediction<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p,
                                  obs_eve=individu_pris,
                                  epsilon_t_plus1=unlist(Simul_epsi[j,]),
                                  Modele_ar_par_t = Mod_AR))
    return(Prediction)
}
```


```{r}
nom_variable<-"Surcote"
name_for_file<-ifelse(nom_variable=="Surcote","S",no = nom_variable)
fich_reg1<-paste0("residuals/",name_for_file,"_Reg_AR1.csv")

fichier_Simulation_residus<-paste0("residuals/Simulations_",nom_variable,"_avec_std.csv")
fichier_idx_extremes<-paste0("inds_extremes_donnees_",nom_variable,".csv")
dates_Johanna<-c("10/03/2008","08/03/2008", 
                 "09/03/2008")
```

```{r}
nom_recherche<-paste0("data_detrend_Winter/",nom_variable,"_detrend.csv")

# Observer la norme des résidus -------------------------------------------
Residus<-read.csv(file = "residuals/S_residuals.csv")[,2:38]
colnames(Residus)<-c(1:ncol(Residus))

NL2_residus<-apply(X = Residus,MARGIN = 1,FUN = calcul_norme_L2)
obs<-read.csv(nom_recherche)[,2:38]
colnames(obs)<-c(1:37)

# Import des individus extrêmes dans la base ------------------------------
fichier_idx_extremes<-paste0("residuals/",fichier_idx_extremes)
Inds_extremes_L2<-read.csv(fichier_idx_extremes)[,2]
Individus_extremes_base<-obs[Inds_extremes_L2,]
rownames(Individus_extremes_base)<-c(1:nrow(Individus_extremes_base))
NL2_residus_exts<-NL2_residus[Inds_extremes_L2]

# Import dates ------------------------------------------------------------
Dates_prises<-read.csv(file="data_detrend_Winter/dates.csv")[,2]
Dates_prises_extremes<-Dates_prises[Inds_extremes_L2]
mini_date<-substr(Dates_prises_extremes,1,10)
indice_Joh<-which(mini_date%in%dates_Johanna)
date_J_found<-Dates_prises_extremes[indice_Joh]
# On prend la dernière obs ------------------------------------------------
Simulations_epsi<-read.csv(file=fichier_Simulation_residus)[,2:38]
NL2_epsi_simul<-apply(X=Simulations_epsi,
                      MARGIN=1,FUN = calcul_norme_L2)
par(mfrow=c(1,2))
colnames(Simulations_epsi)<-c(1:37)
matplot(t(Simulations_epsi),type="l")
matplot(t(Residus[Inds_extremes_L2,]),type="l")
par(mfrow=c(1,1))
```


```{r}
# Eventual relation between  epsilon_t et residus -------------------------------------------
number_obs<-nrow(obs)-1
df_resultat<-matrix(NA,nrow = 37,ncol = 4)
DF_EV_resid<-read.csv(file=paste0("Work_RVariations/EVA_",nom_variable,"_residus.csv"))[,2:5]
vect_threshold_GPD<-DF_EV_resid$seuil_t

# Correlation_t=19 --------------------------------------------------------
val_peak<-obs[1:number_obs,19]
r_peak<-Residus[c(2:nrow(obs)),19]

for(t in c(1:ncol(obs))){

  # subset of extreme/Mann Whitney ------------------------------------------
  seuil_t<-vect_threshold_GPD[t]
  series_r<-Residus[c(2:nrow(obs)),t]
  veille_r<-obs[1:number_obs,t]
  indices_ext<-which(series_r>seuil_t)
  indice_non_ext<-which(series_r<=seuil_t)
  subset1<-veille_r[indices_ext]
  subset2<-veille_r[indice_non_ext]

  # Correlation test --------------------------------------------------------

  Pearson_t<-cor.test(x = series_r,y=veille_r,method="pearson")$p.value
  Spearman_t<-cor.test(x = series_r,y=veille_r,method="spearman")$p.value
  Kendall_t<-cor.test(x = series_r,y=veille_r,method="kendall")$p.value
  MaWhitney_U<-wilcox.test(subset1,subset2)$p.value
  df_resultat[t,]<-c(Pearson_t, Spearman_t,
                     Kendall_t,MaWhitney_U)
}

# Modele AR ---------------------------------------------------------------
Mod_AR<-read.csv(file=fich_reg1)
Fin<-ncol(Mod_AR)
Mod_AR<-Mod_AR[,c(2:Fin)]

M<-nrow(Simulations_epsi)
Y_Mat<-matrix(NA,nrow = M,ncol = 37)
echantillonnage_YO<-function(niv_simul,echs_N2){
  return(which.min(abs(niv_simul-echs_N2)))
}
# Se baser sur des obs non exts -------------------------------------------
Indices_non_ext<-which(!c(1:nrow(obs))%in%Inds_extremes_L2==TRUE)

NL2<-apply(X = obs,MARGIN = 1,FUN = calcul_norme_L2)
inds_non_nuls<-which(NL2_residus>0)
plot(NL2_residus[inds_non_nuls],NL2[inds_non_nuls],
     main="Relation des deux normes", 
     xlab=expression("Norme de epsilon"[M]),ylab=expression("Norme de Y"[M]))
```

```{r}
y<-NL2[inds_non_nuls]
x<-NL2_residus[inds_non_nuls]
modele_lin_epsi_Y<-lm(y~x)
JBtest<-tseries::jarque.bera.test(modele_lin_epsi_Y$residuals)
seuil_y<-quantile(y,0.75)
seuil_x<-quantile(x,0.75)
number_obs<-length(NL2)-1
NL2_veille<-NL2[1:number_obs]
filtre<-min(NL2[Inds_extremes_L2])

prob_non_exceedance<-mean(as.numeric(NL2<=filtre))
print("Return period")
npy<-length(NL2)/37
periode_retour<-(npy*(1-prob_non_exceedance))^(-1)
Indices_non_ext<-which(NL2<filtre)
par(mfrow=c(1,1))
N_XO<-c()
df_relation_dsbase<-cbind.data.frame(NL2_residus[2:length(NL2_residus)],
                                     NL2_veille)

colnames(df_relation_dsbase)<-c("epsi","L2_indu_pris")


```


```{r}
inds_chosen<-which((df_relation_dsbase$epsi>0))
df_subset<-df_relation_dsbase[inds_chosen,]
## (3) Choice of the previous time series

ech_fenetre<-sapply(X =NL2_epsi_simul,Sample_window,
         y=NL2_veille,x_window=NL2_residus[2:length(NL2_residus)],size_window=20)
Echantillons_pris<-ech_fenetre
```

```{r}
L<-nrow(obs)-1
sub_obs<-NL2[1:L]
sub_resid<-NL2_residus[2:nrow(obs)]
sub_obs<-sub_obs[which(sub_resid>0)]
sub_resid<-subset(sub_resid,sub_resid>0)
df<-cbind.data.frame(sub_obs,sub_resid)
colnames(df)<-c("obs_passe","resid")
modele_choix<-lm(obs_passe~resid,data = df)
predictions_niveaux_passe<-predict(modele_choix,data.frame(resid=NL2_epsi_simul))

par(mfrow=c(1,2))
plot(density(NL2[Echantillons_pris]),main="norme L2 des obs (X0) prises")
plot(density(NL2[Inds_extremes_L2-1]),main="norme L2 veille des obs extremes")
par(mfrow=c(1,1))
nb_decal<-0
if(ncol(Mod_AR)>2){
  nb_decal<-ncol(Mod_AR)-2
}
Y_Mat<-t(sapply(c(1:M),Simulate_extreme_from_resid,nb_decal = nb_decal,
                     Sample_taken = Echantillons_pris ,
                     Mod_AR = Mod_AR,Simul_epsi = Simulations_epsi,
                     obs = obs))
Y_Mat<-as.data.frame(Y_Mat)
colnames(Y_Mat)<-c(1:ncol(Y_Mat))
NL2_simulations<-apply(X =  Y_Mat,MARGIN = 1,FUN=calcul_norme_L2)
print("simul vs veille--YO")
test<-Inds_extremes_L2[2]-1
```


```{r}

# Simulation evenement type Johanna ---------------------------------------
Veille_Johann<-obs[Inds_extremes_L2[indice_Joh]-1,]
date_veille_J<-Dates_prises[Inds_extremes_L2[indice_Joh]-1]
Johann<-obs[Inds_extremes_L2[indice_Joh],]
Next_day_extreme<-obs[Inds_extremes_L2[indice_Joh]+1,]
M_Prediction_obs_J<-t(sapply(c(1:nrow(Simulations_epsi)),
                            FUN = function(x)
    {vecteur<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p,
                                                   obs_eve=Veille_Johann,epsilon_t_plus1=unlist(Simulations_epsi[x,]),
                                                   Modele_ar_par_t = Mod_AR))
     return(vecteur)}))
M_Prediction_consecutive_extreme<-t(sapply(c(1:nrow(Simulations_epsi)),
                           FUN = function(x)
                           {vecteur<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p,obs_eve=Johann,             epsilon_t_plus1=unlist(Simulations_epsi[x,]),Modele_ar_par_t = Mod_AR))
                           return(vecteur)}))

# versus observation normal -----------------------------------------------

X_zero_nomal<-obs[Echantillons_pris[1],]
date_XOnormal<-Dates_prises[Echantillons_pris[1]]
Next_day_normal<-obs[Echantillons_pris[1]+1,]

M_Prediction_normal_day<-t(sapply(c(1:nrow(Simulations_epsi)),
         FUN = function(x){vecteur<-as.numeric(sapply(X = c(1:ncol(obs)),FUN = Function_AR_p, obs_eve=X_zero_nomal,epsilon_t_plus1=unlist(Simulations_epsi[x,]),
          Modele_ar_par_t = Mod_AR))
               return(vecteur)}))

```

```{r}
# Relation veille_XO vs norme epsilon -------------------------------------
NL2_epsi_simul<-apply(X = Simulations_epsi,MARGIN = 1,
                      FUN = calcul_norme_L2)
df_residus_vs_veille_simul<-cbind.data.frame(NL2_epsi_simul,NL2[Echantillons_pris])
colnames(df_residus_vs_veille_simul)<-c("simul_epsi","L2_indu_pris")
df_residus_vs_veille_simul$indicatrice<-sapply(Echantillons_pris,function(x){return(x%in%Inds_extremes_L2)})

  # Simulations -------------------------------------------------------------
compar_pic<-cbind.data.frame(r_peak,val_peak)
colnames(compar_pic)<-c("r_peak","val_peak")
GG_rel_v2<-ggplot(data=df_residus_vs_veille_simul,aes(x=simul_epsi,
                                                      y=L2_indu_pris,col="simulations"))+
  geom_point(pch=17)+
  xlab(expression("L2 norm of "~epsilon[M]))+
  ylab(expression("L2 norm of "~tilde(X)[M-1]))+
  geom_point(data = df_relation_dsbase,aes(y=L2_indu_pris,
                                           x=epsi,col="data"))+
  scale_color_manual(values=c("simulations"="orange","data"="blue"))+
  labs(col="Legend")+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))

print(GG_rel_v2)
```


```{r}
L_predJ<-list("simulations"=M_Prediction_obs_J,
              "X0"=Veille_Johann, 
              "date_X0"=date_veille_J, 
              "X1"=Johann, 
              "date_Johanna"=date_J_found)
lJ<-list("simulations"=M_Prediction_consecutive_extreme,
         "X0"=Johann, 
         "date_J"=date_J_found,
         "X1"=Next_day_extreme)
lnormal<-list("simulations"=M_Prediction_normal_day,
              "X0"=X_zero_nomal, 
              "date_XO"=date_XOnormal,
              "X1"=Next_day_normal)
```

```{r}
resultat_Simul_Obs<-list("simulations_AR"=Y_Mat,"observations_exts_base"=Individus_extremes_base,"dates_evenements_extremes"=Dates_prises_extremes,
   "liste_Johanna"=lJ,"liste_normal"=lnormal,
   "X"=obs,"indices_X0"=Echantillons_pris, 
   "indices_exts"=Inds_extremes_L2,"liste_Pred_Johanna"=L_predJ)
```

```{r}

## Data-like time series or consecutive extremes
Obs_ext<-resultat_Simul_Obs$observations_exts_base
Simul<-resultat_Simul_Obs$simulations_AR
Inds_drawn<-sample(c(1:nrow(Obs_ext)),size = nrow(Obs_ext)/3,
       replace = FALSE)
```


#(III) Consistency of our simulations. 
##(1) Quantiles value

```{r}
N_rep<-500
B<-nrow(Individus_extremes_base)
estimateur_95<-apply(Individus_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.95)))})
estimateur_975<-apply(Individus_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.975)))})
estimateur_05<-apply(Individus_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.05)))})
estimateur_50<-apply(Individus_extremes_base,MARGIN =2,function(x){return(as.numeric(quantile(x,0.50)))})

# estimateur simulations --------------------------------------------------
estimateur_95_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.95)))})
estimateur_05_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.05)))})
estimateur_50_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.50)))})
estimateur_975_simul<-apply(Y_Mat,MARGIN =2,function(x){return(as.numeric(quantile(x,0.975)))})
L_q<-c(0.95,0.975,0.05,0.50)
Tendencies<-replicate(n =N_rep,expr = Resamples_tendencies(B = B,extremes_indus = Individus_extremes_base,
                                                    list_Q = L_q))
borne_plus95<-apply(Tendencies[1,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
borne_moins95<-apply(Tendencies[1,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})

borne_plus975<-apply(Tendencies[2,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
borne_moins975<-apply(Tendencies[2,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})

borne_plus05<-apply(Tendencies[3,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
borne_moins05<-apply(Tendencies[3,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})

borne_plus50<-apply(Tendencies[4,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.975)))})
borne_moins50<-apply(Tendencies[4,,],MARGIN = 1,FUN = function(x){return(as.numeric(quantile(x,0.025)))})
```


```{r}
df<-cbind.data.frame(c(estimateur_05,estimateur_50,estimateur_95,estimateur_975),
                     c(borne_moins05,borne_moins50,
                       borne_moins95,borne_moins975),
                     c(borne_plus05,borne_plus50,
                       borne_plus95,borne_plus975))
colnames(df)<-c("estimateur","borne_moins","borne_plus")

df$estimateur_simul<-c(estimateur_05_simul,estimateur_50_simul
                       ,estimateur_95_simul,estimateur_975_simul)
df$Temps<-rep(c(1:ncol(Individus_extremes_base)),4)
df$Temps<-(df$Temps-19)*(1/6)
df$percent<-c(rep("Q05",ncol(Individus_extremes_base)),
               rep("Q50",ncol(Individus_extremes_base)),
               rep("Q95",ncol(Individus_extremes_base)), 
               rep("Q975",ncol(Individus_extremes_base)))
GG_percent<-ggplot(data=df,aes(x=Temps,y=estimateur,group=interaction(percent),col="data"))+
  facet_wrap(~percent,scales="free_y")+
  geom_line()+
  geom_point()+
  geom_ribbon(mapping = aes(ymin=borne_moins,ymax=borne_plus,col="confidence_band"),alpha=0.15,
              fill="grey", linetype = "dashed")+
  geom_line(aes(x=Temps,y=estimateur_simul,col="simulations"),linetype=2)+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))+
  ylab("Surge (m)")+
  scale_color_manual(values=cols_)+
  xlab("Time (hour) with respect to tidal peak")+
  labs(col="Legend")
print(GG_percent)
 
```

##(2) Functional decomposition of simulated and recorded time series

```{r}
NL2_ext<-apply(X = Obs_ext,MARGIN = 1,FUN = calcul_norme_L2)
NL2_simul<-apply(X = Simul,MARGIN = 1,FUN = calcul_norme_L2)
```

```{r}
inds_non_nuls<-which(NL2_simul>0)
Angle_inds<-t(t(Obs_ext)%*%diag(NL2_ext^(-1)))
Angle_simul<-t(t(Simul[inds_non_nuls,])%*%diag(NL2_simul[inds_non_nuls]^(-1)))
PCA_inds<-FactoMineR::PCA(X = Angle_inds,scale.unit = TRUE, 
                          graph = FALSE)
mu_angle<-colMeans(Angle_inds)
percent_variance<-PCA_inds$eig[c(1:2),2]
plot(c(100,PCA_inds$eig[,2]),type="o"
     ,main=paste0("Evolution of the ratio of unexplained inertia for ",ifelse(nom_variable=="Surcote","S",nom_variable)))
sd_angle<-apply(X =Angle_inds,MARGIN = 2,FUN = sd)
```

```{r}
resultat<-scale(Angle_simul,center = mu_angle,
                scale = sd_angle)
coords_donnees<-PCA_inds$ind$coord[,c(1:2)]
conversion_donnees<-resultat%*%PCA_inds$svd$V[,c(1:2)]
ks.test(conversion_donnees[,1],coords_donnees[,1])
wilcox.test(conversion_donnees[,1],coords_donnees[,1])

ks.test(conversion_donnees[,2],coords_donnees[,2])
wilcox.test(conversion_donnees[,2],coords_donnees[,2])
```

```{r}
df_simul_forme<-cbind.data.frame(conversion_donnees)
colnames(df_simul_forme)<-sapply(c(1:ncol(df_simul_forme)), 
                               function(x){return(paste0("Score_",x))})

df_data_forme<-as.data.frame(coords_donnees)
colnames(df_data_forme)<-sapply(c(1:ncol(df_data_forme)), 
                              function(x){return(paste0("Score_",x))})
df_combo<-rbind.data.frame(df_data_forme,df_simul_forme)
df_combo$Legend<-c(rep("data",nrow(df_data_forme)),
                 rep("simulations",nrow(df_simul_forme)))
GG1<-ggplot(data=df_combo,aes(x=Score_1,y=Score_2,colour=Legend,
                              shape=Legend))+
  geom_point(aes(size=Legend))+
  scale_size_manual(values=c("simulations"=0.75,"data"=1.5))+
  geom_xsidedensity(data=df_combo,aes(fill=Legend), alpha = 0.5)+
  geom_ysidedensity(data=df_combo,aes(fill=Legend), alpha = 0.5)+
  xlab(paste0("First dimension (",round(percent_variance[1],1),"% of the variance)"))+
  ylab(paste0("Second dimension (",round(percent_variance[2],1),"% of the variance)"))+
  scale_color_manual(values=cols_)+
  scale_fill_manual(values=cols_)+
  scale_shape_manual(values = c("simulations"=17,"data"=19))+
  guides(fill="none")+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))
GG1

```

## (3) Correlation of extreme values
```{r}
# Comparaison des extremogrammes ------------------------------------------
Matrice_couples<-list()
L<-ncol(Y_Mat)
z<-1
vecteur_distances<-c()
for(j in 1:L){
  #condition imposée sur le deuxième temps.
  valeurs_t_plus_h<-j:L
  for (i in valeurs_t_plus_h){
    Matrice_couples[[z]]<-c(i,j)
    vecteur_distances<-c(vecteur_distances,abs(i-j))
    z<-z+1
  }
}
q_chosen<-0.90
Tau1<-sapply(c(1:37),function(x,q,t){
  return(as.numeric(quantile(x[,t],q)))},q=q_chosen,
  x=Y_Mat)
Tau2<-sapply(c(1:37),function(x,q,t){
  return(as.numeric(quantile(x[,t],q)))},q=q_chosen,
  x=Individus_extremes_base)
Extremogram_simulations<-empirical_extremogram(Matrix_couples = Matrice_couples,inds_select = Y_Mat,Tau = Tau1)
Extremogram_donnees<-empirical_extremogram(Matrix_couples = Matrice_couples,inds_select= Individus_extremes_base,Tau = Tau2)


Resultat<-replicate(n =N_rep,expr = fnct_estim_extremo_resample(B = B,inds_ext  = Individus_extremes_base,Tau = Tau2,Matrix_couples = Matrice_couples,vector_distances=vecteur_distances))
```


```{r}
fnct_quantile<-function(x,q){
  M<-apply(X = x,MARGIN = 2,FUN = function(X){return(as.numeric(quantile(X,q)))})
  return(M)
}
Q05<-fnct_quantile(x = t(Resultat),q=0.025)
Q95<-fnct_quantile(x = t(Resultat),q=0.975)
df<-cbind.data.frame(Extremogram_simulations,Extremogram_donnees,vecteur_distances)
colnames(df)<-c("valeur_simulation","valeur_donnees","delta")

resultat_delta<-df %>% group_by(delta) %>% summarise(val_donnees=mean(valeur_donnees),
                                                     val_simul=mean(valeur_simulation))
resultat_delta$borne_inf<-Q05
resultat_delta$borne_sup<-Q95
travail_extremo<-as.data.frame(resultat_delta[,c("val_donnees","val_simul","borne_inf","borne_sup")])
travail_extremo$Temps<-c(1:nrow(travail_extremo))
GG_extremo<-ggplot(travail_extremo,aes(x=Temps,y=val_donnees,col="data"))+
  geom_line()+
  geom_point()+
  geom_line(aes(y=val_simul,col="simulations"))+
  geom_point(aes(y=val_simul,col="simulations"),pch=2)+
  geom_ribbon(mapping = aes(ymin=borne_inf,ymax=borne_sup,col="confidence_band"),alpha=0.15,
              fill="grey", linetype = "dashed")+
  scale_color_manual(values=cols_)+
  theme(axis.title=element_text(size=15),
        legend.text=element_text(size=10))+
  labs(col="Legend")+
  xlab("Lag h")+
  ylab("value")

print(GG_extremo)

```

## 3) EVA analysis
```{r}
if(nom_variable=="Hs"){
  Y_graph_lim<-c(4,11)
}
if(nom_variable=="Surcote"){
  Y_graph_lim<-c(0.13,1)
}
Indices_exts<-read.csv(paste0(fichier_idx_extremes))[,2]
p_u<-rep(0.05,ncol(Simul))
periods_years<-c(2,5,10,20,50,80,
                 100,120,200,
                 250,300,500,
                 800)
# Define the larger return period according to the number 
# of years that we used, here 37
NB_years<-37
limite_sup<-3*NB_years
periods_years<-periods_years[which(periods_years<limite_sup)]

```

```{r}
for(t in c(13,19,25,31)){
  lag<-t-19
  comment<-ifelse(lag<0,paste0(abs(round(lag/6)),"h before"),
                  ifelse(lag==0," at",paste0(round(lag/6),"h after")))
  series_simul<-Simul[,t]
  series_t<-resultat_Simul_Obs$X[,t]
  seuil<-quantile(series_t,1-p_u[t])
  
  series_ext<-subset(series_t,series_t>seuil)
  NB_years_total<-37
  npy<-length(series_t)/NB_years_total
  Nom_graph<-ifelse(nom_variable=="Surcote","S",
                    nom_variable)
  RL_ggplot_cond_ext(series = series_t,seuil = seuil,
                     period_years = periods_years,
                     NPY = round(npy),
                     plus_simul = TRUE,
                     series_simul = series_simul, 
                     titre = paste0("Return level (ML) ",comment, " the tidal peak for ",Nom_graph), 
                     nom_variable = nom_variable, 
                     cols_ggplot = cols_, 
                     unit_used = "m",
                     ylim_opt = Y_graph_lim, 
                     alpha=0.05,
                     Individus_exts=Indices_exts)
}
```



```{r}
norm<-as.numeric(resultat_Simul_Obs$liste_normal$X0)
simul_Xoest_normal<-melt(t(resultat_Simul_Obs$liste_normal$simulations[Inds_drawn,]))
colnames(simul_Xoest_normal)<-c("time","id","value")
simul_Xoest_normal$id<-as.character(simul_Xoest_normal$id)
simul_Xoest_normal$X_zero<-norm[simul_Xoest_normal$time]
simul_Xoest_normal$time<-(simul_Xoest_normal$time-19)/6
Date_normal<-resultat_Simul_Obs$liste_normal$date_XO
X1_normal<-as.numeric(resultat_Simul_Obs$liste_normal$X1)
GG_normal<-ggplot(data=simul_Xoest_normal,aes(x=time,y=value,
                            group=interaction(id), 
                            col=id))+
  geom_line(alpha=0.7,show.legend=FALSE)+
  xlab(" ")+ylab(" ")+
  geom_line(aes(y=X_zero),col="red", 
            linetype="dotted",size=1.2)+
  annotate("text", x = 0, y = X1_normal[30], 
           label=Date_normal)
```


```{r}
simul_Xoest<-resultat_Simul_Obs$liste_Johanna$simulations[Inds_drawn,]
Joh<-as.numeric(resultat_Simul_Obs$liste_Johanna$X0)
date_X0ext<-resultat_Simul_Obs$liste_Johanna$date_J
simul_Xoest<-melt(t(simul_Xoest))
X1_ext<-as.numeric(resultat_Simul_Obs$liste_Johanna$X1)
colnames(simul_Xoest)<-c("time","id","value")
simul_Xoest$id<-as.character(simul_Xoest$id)
simul_Xoest$X_zero<-Joh[simul_Xoest$time]
simul_Xoest$X_un<-X1_ext[simul_Xoest$time]
simul_Xoest$time<-(simul_Xoest$time-19)/6
GGJoh<-ggplot(data=simul_Xoest,aes(x=time,y=value,
                            group=interaction(id), 
                            col=id))+
  geom_line(alpha=0.7)+
  guides(col="none")+
  xlab(" ")+ylab(" ")+
  geom_line(aes(y=X_zero),col="red", 
            linetype="dotted",size=1.2)+
  labs(linetype="Date of X0")+
  annotate("text", x = 0, y = X1_ext[30], 
           label=date_X0ext)


```

```{r}
min_grid<-min(min(simul_Xoest$value), min(simul_Xoest_normal$value), 
              min(simul_Xoest$X_zero),min(simul_Xoest_normal$X_zero), 
              X1_normal[30],X1_ext[30])
max_grid<-max(max(simul_Xoest$value), max(simul_Xoest_normal$value), 
              max(simul_Xoest$X_zero),max(simul_Xoest_normal$X_zero),
              X1_normal[30],X1_ext[30])
GG_normal<-GG_normal+ylim(c(min_grid,max_grid))
GGJoh<-GGJoh+ylim(c(min_grid,max_grid))
```

```{r}
nom_present<-ifelse(nom_variable=="Surcote",yes = "S",no = nom_variable)
yleft <- textGrob(paste0(nom_present," (m)"),
                  rot=90,
                  gp = gpar(col = "black", fontsize = 15))
Xbottom<-textGrob("Time (hour) with respect to tidal peak",
                  gp = gpar(col = "black", fontsize = 15))

gridExtra::grid.arrange(GGJoh,GG_normal,ncol=2,
                        left=yleft,bottom=Xbottom)
```


##3) Classification two-samples test
```{r}
source("fonctions/fonctions_perfs_ML.R")
racine_ML<-c("results_ML/")
H_Params<-c("radial","logit",500)
NB_times<-100
ALPHA_PROP<-0.10
K<-4
typeS<-"simple"
resultat<-matrix(NA,ncol=3,nrow=3)

```

```{r}
Lperfs_Y<-Running_perfs_ML(Base_simul = Simul,Base_data = Obs_ext, 
                   hyp_param =H_Params,
                   NB_times=100,alpha_prop=ALPHA_PROP,K = K, 
                   type_sampling = typeS,title_ROC =nom_variable,
                   NB_shown_ROC = 20)
resultat[1,]<-Lperfs_Y$Accuracy$qu_accuracy
```

```{r}
Lperfs_L2<-Running_perfs_ML(Base_simul = NL2_simul,Base_data = NL2_ext, 
                              hyp_param = H_Params, NB_times = NB_times,alpha_prop=ALPHA_PROP, 
                              K=K,type_sampling = typeS
                           ,title_ROC = paste0(nom_variable," Cost-f"),NB_shown_ROC = 20)
resultat[2,]<-Lperfs_L2$Accuracy$qu_accuracy
```

```{r}
Lperfs_A_f<-Running_perfs_ML(Base_simul = Angle_simul,Base_data = Angle_inds, hyp_param =H_Params , NB_times = NB_times,alpha_prop=ALPHA_PROP,K=K,type_sampling = typeS, title_ROC = paste0(nom_variable," angle"),NB_shown_ROC = 20)

resultat[3,]<-Lperfs_A_f$Accuracy$qu_accuracy
```

```{r}
resultat
```

```{r}
## End of the code
```

